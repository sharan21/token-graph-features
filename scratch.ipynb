{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "\n",
    "class TokenGraphEmbedding():\n",
    "    def __init__(self, load_path=None, save_path=\"default.pkl\"):\n",
    "        self.num_nodes = 0\n",
    "        self.tot_weight = 0\n",
    "        self.all_tokens = []  \n",
    "        self.adj = {} \n",
    "        self.save_path = save_path\n",
    "        self.w2i = {}\n",
    "\n",
    "        if(load_path):\n",
    "            self.load(path=load_path)\n",
    "\n",
    "    def clean_line(self, line, bad=[',','.', ';', '(', ')', '/', '`', '%', '\"', '-', '\\\\','\\'',]):\n",
    "        clean = ''\n",
    "        for c in line:\n",
    "            if c not in bad:\n",
    "                clean += c\n",
    "        return clean\n",
    "    \n",
    "    def save(self, path=\"default.pkl\"):\n",
    "        with open(os.path.join(\"./saved\",path), 'wb') as handle:\n",
    "            pickle.dump(self, handle)\n",
    "    \n",
    "    def init_from_obj(self,b):\n",
    "        self.num_nodes = b.num_nodes\n",
    "        self.all_tokens = b.all_tokens\n",
    "        self.adj = b.adj\n",
    "        self.tot_weight = b.tot_weight\n",
    "    \n",
    "    def load(self, path=\"default.pkl\"):\n",
    "        with open(os.path.join(\"./saved\",path), 'rb') as handle:\n",
    "            b = pickle.load(handle)\n",
    "        self.init_from_obj(b)\n",
    "\n",
    "    def display(self, print_adj=False):\n",
    "        print(\"num_nodes: {}\".format(self.num_nodes))\n",
    "        print(\"tot_weight: {}\".format(self.tot_weight))\n",
    "        print(\"all_tokens: {}\".format(self.all_tokens))\n",
    "        \n",
    "        if(print_adj):\n",
    "            print(\"\\nAdjacency Matrix: \\n\")\n",
    "            for token in self.all_tokens:\n",
    "                print(token)\n",
    "                for neigh in self.adj[token]:\n",
    "                    print(\"\\t {} : {}\".format(neigh,self.adj[token][neigh]))\n",
    "                print()\n",
    "\n",
    "    def update_link(self, token_src, token_dst, weight, link_type=\"bidirectional\",stop=False):\n",
    "        #check if this token exists\n",
    "        if(token_src not in self.adj):\n",
    "            self.adj[token_src] = {}\n",
    "            self.all_tokens.append(token_src)\n",
    "            self.w2i[token_src] = self.num_nodes\n",
    "            self.num_nodes += 1\n",
    "        #check if prior link exists\n",
    "        if token_dst not in self.adj[token_src]:\n",
    "            self.adj[token_src][token_dst] = 0\n",
    "        # update dst -> src link\n",
    "        self.adj[token_src][token_dst] += weight     \n",
    "        self.tot_weight += weight   \n",
    "        if(link_type==\"bidirectional\"):\n",
    "            if(not stop):\n",
    "                self.update_link(token_dst, token_src, weight, stop=True)\n",
    "\n",
    "    def update_graph(self, from_file):\n",
    "        with open(from_file, \"r\") as f:\n",
    "            sents = f.readlines()\n",
    "        for sent in sents:\n",
    "            sent = self.clean_line(sent)\n",
    "            tokens = sent.split(' ')\n",
    "            for i in tqdm(range(len(tokens))):\n",
    "                if(i == 0):\n",
    "                    continue\n",
    "                self.update_link(token_src=tokens[i], token_dst=tokens[i-1], weight=1)\n",
    "        \n",
    "        trythis.save()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_nodes: 55\n",
      "tot_weight: 992\n",
      "all_tokens: ['allows', 'PPLM', 'a', 'user', 'to', 'flexibly', 'plug', 'in', 'one', 'or', 'more', 'tiny', 'attribute', 'models', 'representing', 'the', 'desired', 'steering', 'objective', 'into', 'large', 'unconditional', 'language', 'model', 'LM', 'The', 'method', 'has', 'key', 'property', 'that', 'it', 'uses', 'as', 'is—no', 'training', 'finetuning', 'is', 'required—which', 'enables', 'researchers', 'leverage', 'bestinclass', 'LMs', 'even', 'if', 'they', 'do', 'not', 'have', 'extensive', 'hardware', 'required', 'train', 'them']\n"
     ]
    }
   ],
   "source": [
    "# t = TokenGraphEmbedding(load_path=\"./default/default.pkl\")\n",
    "t = TokenGraphEmbedding()\n",
    "t.update_graph(from_file='./sample.txt')\n",
    "t.display()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_lbls(path):\n",
    "    with open(path, 'r') as f:\n",
    "        with open(path + '.cleaned', 'w') as f2:\n",
    "            for line in f:\n",
    "                line = line.split(',')[0]\n",
    "                f2.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_lbls('./data/annotated/yelp-new/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
